{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5619aa29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T11:59:41.573179Z",
     "start_time": "2022-04-26T11:59:37.975902Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "from tensorflow.keras.layers import Flatten, Dense,Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c973301d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T11:59:41.759394Z",
     "start_time": "2022-04-26T11:59:41.590093Z"
    }
   },
   "outputs": [],
   "source": [
    "DATADIR = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464edf4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:00.487401Z",
     "start_time": "2022-04-26T11:59:41.760391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10716 files belonging to 150 classes.\n",
      "Using 8573 files for training.\n",
      "Found 10716 files belonging to 150 classes.\n",
      "Using 2143 files for validation.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "IMG_SIZE = 300\n",
    "train = image_dataset_from_directory(DATADIR,\n",
    "                                     labels=\"inferred\",\n",
    "                                     label_mode=\"categorical\",\n",
    "                                     color_mode=\"rgb\",\n",
    "                                     batch_size=256,\n",
    "                                     image_size=(\n",
    "                                         IMG_SIZE, IMG_SIZE),\n",
    "                                     shuffle=True,\n",
    "                                     seed=777,\n",
    "                                     validation_split=0.2,\n",
    "                                     subset=\"training\")\n",
    "val = image_dataset_from_directory(DATADIR,\n",
    "                                   labels=\"inferred\",\n",
    "                                   label_mode=\"categorical\",\n",
    "                                   color_mode=\"rgb\",\n",
    "                                   batch_size=256,\n",
    "                                   image_size=(\n",
    "                                       IMG_SIZE, IMG_SIZE),\n",
    "                                   shuffle=True,\n",
    "                                   seed=777,\n",
    "                                   validation_split=0.2,\n",
    "                                   subset=\"validation\")\n",
    "test = train.skip(int(0.75*len(train)))\n",
    "train = train.take(int(0.75*len(train)))\n",
    "test = test.take(int(0.25*len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ac7b36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:00.582518Z",
     "start_time": "2022-04-26T12:06:00.489401Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=train.repeat(count=7)\n",
    "train=train.shuffle(buffer_size=128)\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f990894",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:01.660318Z",
     "start_time": "2022-04-26T12:06:00.584284Z"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.4),\n",
    "        layers.experimental.preprocessing.RandomZoom(0.5),\n",
    "        layers.experimental.preprocessing.RandomContrast(0.3),\n",
    "        layers.experimental.preprocessing.Rescaling(1./255)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba14bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:01.723100Z",
     "start_time": "2022-04-26T12:06:01.663270Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train = train.prefetch(buffer_size=AUTOTUNE)\n",
    "val = val.prefetch(buffer_size=AUTOTUNE)\n",
    "test = test.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255b421c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:07.276161Z",
     "start_time": "2022-04-26T12:06:01.726098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f19a3729c40> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38003a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a3800b80> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f19a3729d60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38832b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a389cf70> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f19a389cc70> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38a23a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38a2f10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a3880ac0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f19a3883fa0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38a95e0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38ae4f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a388ddc0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f19a38a20a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38b74f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a38bdeb0> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f19a388d520> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f19a38b7f40> True\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGG16(weights='imagenet', include_top=False,\n",
    "                  input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "vgg_model.trainable = True\n",
    "\n",
    "trainable_layer = 3\n",
    "for layer in vgg_model.layers[:-trainable_layer]:\n",
    "    layer.trainable = False\n",
    "for layer in vgg_model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73296e43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:11.993706Z",
     "start_time": "2022-04-26T12:06:07.279154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 300, 300, 3)       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 9, 9, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              42468352  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1024)             4096      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 150)               76950     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,790,934\n",
      "Trainable params: 47,792,790\n",
      "Non-trainable params: 9,998,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# Image augmentation block\n",
    "x = data_augmentation(inputs)\n",
    "x = vgg_model(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "outputs = Dense(150, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beb82d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:12.501378Z",
     "start_time": "2022-04-26T12:06:11.995699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f19a38a9a00> True\n",
      "<keras.engine.sequential.Sequential object at 0x7f19a4545a30> True\n",
      "<keras.engine.functional.Functional object at 0x7f19a38bf310> True\n",
      "<keras.layers.core.flatten.Flatten object at 0x7f19a38a97c0> True\n",
      "<keras.layers.core.dense.Dense object at 0x7f19a38bdfa0> True\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f19a36e5b20> True\n",
      "<keras.layers.core.dropout.Dropout object at 0x7f19a3800d60> True\n",
      "<keras.layers.core.dense.Dense object at 0x7f19a3800e80> True\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7f19a36e5820> True\n",
      "<keras.layers.core.dropout.Dropout object at 0x7f19a0df4970> True\n",
      "<keras.layers.core.dense.Dense object at 0x7f19a0e0a640> True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2c4f697",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:46.516564Z",
     "start_time": "2022-04-26T12:06:12.503344Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d32cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T12:06:47.928905Z",
     "start_time": "2022-04-26T12:06:47.619964Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = 'model-logs/mid/vgg16'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "modelfiles = model_dir + '/{}-best-model.h5'.format('basic1_model')\n",
    "model_mckp = keras.callbacks.ModelCheckpoint(modelfiles,\n",
    "                                             monitor='val_accuracy',\n",
    "                                             save_best_only=True)\n",
    "\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                          patience=10,\n",
    "                                          verbose=1)\n",
    "\n",
    "callbacks_list = [model_mckp, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba39c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T19:02:07.092313Z",
     "start_time": "2022-04-26T12:06:47.930861Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "175/175 [==============================] - 164s 652ms/step - loss: 3.6481 - accuracy: 0.2502 - val_loss: 2.5635 - val_accuracy: 0.4251\n",
      "Epoch 2/50\n",
      "175/175 [==============================] - 148s 641ms/step - loss: 2.0717 - accuracy: 0.5221 - val_loss: 1.7616 - val_accuracy: 0.5903\n",
      "Epoch 3/50\n",
      "175/175 [==============================] - 148s 643ms/step - loss: 1.4528 - accuracy: 0.6612 - val_loss: 1.5392 - val_accuracy: 0.6360\n",
      "Epoch 4/50\n",
      "175/175 [==============================] - 148s 641ms/step - loss: 1.1081 - accuracy: 0.7351 - val_loss: 1.4172 - val_accuracy: 0.6547\n",
      "Epoch 5/50\n",
      "175/175 [==============================] - 147s 639ms/step - loss: 0.8838 - accuracy: 0.7862 - val_loss: 1.3070 - val_accuracy: 0.6804\n",
      "Epoch 6/50\n",
      "175/175 [==============================] - 147s 641ms/step - loss: 0.7284 - accuracy: 0.8202 - val_loss: 1.2173 - val_accuracy: 0.6920\n",
      "Epoch 7/50\n",
      "175/175 [==============================] - 148s 642ms/step - loss: 0.5986 - accuracy: 0.8539 - val_loss: 1.2387 - val_accuracy: 0.6934\n",
      "Epoch 8/50\n",
      "175/175 [==============================] - 148s 639ms/step - loss: 0.5122 - accuracy: 0.8735 - val_loss: 1.2055 - val_accuracy: 0.6986\n",
      "Epoch 9/50\n",
      "175/175 [==============================] - 147s 642ms/step - loss: 0.4542 - accuracy: 0.8855 - val_loss: 1.1485 - val_accuracy: 0.7121\n",
      "Epoch 10/50\n",
      "175/175 [==============================] - 148s 642ms/step - loss: 0.3946 - accuracy: 0.8997 - val_loss: 1.1130 - val_accuracy: 0.7266\n",
      "Epoch 11/50\n",
      "175/175 [==============================] - 145s 623ms/step - loss: 0.3378 - accuracy: 0.9155 - val_loss: 1.1383 - val_accuracy: 0.7191\n",
      "Epoch 12/50\n",
      "175/175 [==============================] - 143s 618ms/step - loss: 0.3083 - accuracy: 0.9224 - val_loss: 1.1299 - val_accuracy: 0.7168\n",
      "Epoch 13/50\n",
      "175/175 [==============================] - 143s 618ms/step - loss: 0.2813 - accuracy: 0.9282 - val_loss: 1.2696 - val_accuracy: 0.6860\n",
      "Epoch 14/50\n",
      "175/175 [==============================] - 146s 636ms/step - loss: 0.2603 - accuracy: 0.9342 - val_loss: 1.1398 - val_accuracy: 0.7037\n",
      "Epoch 15/50\n",
      "175/175 [==============================] - 147s 638ms/step - loss: 0.2449 - accuracy: 0.9370 - val_loss: 1.1656 - val_accuracy: 0.7154\n",
      "Epoch 16/50\n",
      "175/175 [==============================] - 147s 641ms/step - loss: 0.2297 - accuracy: 0.9401 - val_loss: 1.1174 - val_accuracy: 0.7340\n",
      "Epoch 17/50\n",
      "175/175 [==============================] - 144s 618ms/step - loss: 0.2152 - accuracy: 0.9428 - val_loss: 1.1536 - val_accuracy: 0.7336\n",
      "Epoch 18/50\n",
      "175/175 [==============================] - 143s 618ms/step - loss: 0.2035 - accuracy: 0.9453 - val_loss: 1.2150 - val_accuracy: 0.7144\n",
      "Epoch 19/50\n",
      "175/175 [==============================] - 146s 635ms/step - loss: 0.1884 - accuracy: 0.9494 - val_loss: 1.1767 - val_accuracy: 0.7242\n",
      "Epoch 20/50\n",
      "175/175 [==============================] - 147s 637ms/step - loss: 0.1858 - accuracy: 0.9514 - val_loss: 1.1671 - val_accuracy: 0.7275\n",
      "Epoch 20: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    batch_size=256,\n",
    "    epochs=50,\n",
    "    validation_data=val,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "683fc3e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-26T19:08:53.182446Z",
     "start_time": "2022-04-26T19:02:07.094074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 10s 566ms/step - loss: 0.3515 - accuracy: 0.9076\n",
      "\n",
      " Accuracy:0.9075520634651184\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath=modelfiles)\n",
    "score = model.evaluate(test, verbose=1)\n",
    "print(\"\\n Accuracy:{}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363f32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
